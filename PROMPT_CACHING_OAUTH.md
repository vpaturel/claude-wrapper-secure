# üíæ Prompt Caching - OAuth Documentation

**Date** : 2025-11-05
**M√©thode** : Extrapolation depuis API Key + Beta features
**√âtat** : 35% document√© (support OAuth tr√®s incertain)

---

## üìã Vue d'Ensemble

**Prompt Caching** permet de **mettre en cache des parties du prompt** pour :
- R√©duire latence (jusqu'√† 85%)
- R√©duire co√ªt tokens input (jusqu'√† 90%)
- R√©utiliser contexte long (docs, code, instructions)

**Support OAuth** : ‚ö†Ô∏è **TR√àS INCERTAIN** (beta feature, probablement non disponible)

---

## üéØ Fonctionnement (Th√©orique OAuth)

### Concept

**Id√©e** : Cacher portions du prompt r√©utilis√©es fr√©quemment

**Sc√©narios** :
- System prompt identique (instructions longues)
- Documentation technique (docs API, codebase)
- Contexte partag√© (conversation multi-tours)

---

## üîß Structure Requ√™te (Extrapol√©e)

### Header Beta

```http
anthropic-beta: prompt-caching-2024-07-31=true
```

### Format cache_control

```json
{
  "model": "claude-sonnet-4-5-20250929",
  "max_tokens": 1024,
  "system": [
    {
      "type": "text",
      "text": "You are an AI assistant specialized in Python. Here is the complete Python documentation: [LARGE DOC...]",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [
    {
      "role": "user",
      "content": "How do I use asyncio?"
    }
  ]
}
```

**Confiance** : 30% (structure API Key, support OAuth inconnu)

---

## üìä Breakpoints Cache

### R√®gles (Extrapol√©es)

**Cache cr√©√© si** :
- `cache_control` pr√©sent
- Contenu > 1024 tokens (minimum)
- Dur√©e : 5 minutes TTL

**Breakpoints** : Points o√π cache est stock√©

```json
{
  "system": [
    {
      "type": "text",
      "text": "[PARTIE 1 - NON CACH√âE]"
    },
    {
      "type": "text",
      "text": "[PARTIE 2 - CACH√âE]",
      "cache_control": {"type": "ephemeral"}
    }
  ]
}
```

**Seule la derni√®re partie est cach√©e**

---

## üí∞ √âconomies (Estim√©es OAuth)

### Co√ªt Tokens

**Sans cache** :
```
Input : 10,000 tokens √ó $3/M = $0.03
```

**Avec cache (1√®re requ√™te)** :
```
Input (write cache) : 10,000 tokens √ó $3.75/M = $0.0375 (+25% premi√®re fois)
```

**Avec cache (requ√™tes suivantes)** :
```
Input (cache hit) : 10,000 tokens √ó $0.30/M = $0.003 (90% reduction !)
```

**OAuth forfait** : Co√ªt probablement **inclus** (pas de facturation s√©par√©e)

---

## üéØ Use Cases (Si Support√© OAuth)

### 1. System Prompt Long

```json
{
  "system": [
    {
      "type": "text",
      "text": "You are an expert assistant... [5000 tokens instructions]",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [{"role": "user", "content": "Question 1"}]
}
```

**Requ√™te suivante** : R√©utilise cache system prompt

---

### 2. Documentation Technique

```json
{
  "system": [
    {
      "type": "text",
      "text": "Here is the API documentation:\n\n[20,000 tokens doc]",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [{"role": "user", "content": "How do I authenticate?"}]
}
```

---

### 3. Codebase Context

```json
{
  "system": [
    {
      "type": "text",
      "text": "Codebase:\n\n```python\n[15,000 tokens code]\n```",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [{"role": "user", "content": "Where is the auth function?"}]
}
```

---

## üìà Headers Cache (Extrapol√©s)

### Request

```http
POST /v1/messages HTTP/2
anthropic-beta: prompt-caching-2024-07-31=true
```

### Response (Extrapol√©e)

**Headers cache** :
```http
anthropic-cache-creation-input-tokens: 10000
anthropic-cache-read-input-tokens: 0
```

**Usage object** :
```json
{
  "usage": {
    "input_tokens": 50,
    "cache_creation_input_tokens": 10000,
    "cache_read_input_tokens": 0,
    "output_tokens": 200
  }
}
```

**Requ√™te suivante (cache hit)** :
```json
{
  "usage": {
    "input_tokens": 50,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 10000,
    "output_tokens": 180
  }
}
```

---

## üö® Limites Cache (Extrapol√©es)

| Aspect | Limite |
|--------|--------|
| **TTL (Time To Live)** | 5 minutes |
| **Minimum tokens** | 1024 tokens |
| **Maximum tokens** | ~100,000 tokens |
| **Breakpoints max** | ~4 breakpoints |

**Confiance** : 20% (extrapol√© depuis API Key)

---

## üîç Diff√©rences OAuth vs API Key

| Aspect | OAuth | API Key |
|--------|-------|---------|
| **Support** | ‚ùì **TR√àS INCERTAIN** | ‚úÖ Confirm√© (beta) |
| **Header beta** | `anthropic-beta` (extrapol√©) | `anthropic-beta: prompt-caching-2024-07-31=true` ‚úÖ |
| **Structure** | Identique (si support√©) | `cache_control: {type: ephemeral}` ‚úÖ |
| **Co√ªt** | Inclus forfait ? | Factur√© s√©par√©ment ‚úÖ |
| **TTL** | 5 min ? | 5 min ‚úÖ |

**Recommandation** : **Tester avec OAuth** pour confirmer (probablement NON disponible)

---

## üß™ Test Support (√Ä Faire)

### Test Prompt Caching OAuth

```python
import anthropic

client = anthropic.Anthropic()  # OAuth

try:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=100,
        system=[
            {
                "type": "text",
                "text": "You are a helpful assistant. " * 500,  # > 1024 tokens
                "cache_control": {"type": "ephemeral"}
            }
        ],
        messages=[{"role": "user", "content": "Hello"}],
        extra_headers={"anthropic-beta": "prompt-caching-2024-07-31=true"}
    )

    # V√©rifier usage
    usage = response.usage
    if hasattr(usage, 'cache_creation_input_tokens'):
        print("‚úÖ Prompt caching support√© OAuth !")
        print(f"Cache created: {usage.cache_creation_input_tokens} tokens")
    else:
        print("‚ùå Prompt caching NON support√© OAuth")

except Exception as e:
    print(f"‚ùå Erreur: {e}")
```

---

## üéØ Alternative si Non Support√©

### G√©rer Cache C√¥t√© Client

```python
import hashlib

class ClientSideCache:
    def __init__(self):
        self.cache = {}

    def get_cached_response(self, prompt: str):
        key = hashlib.md5(prompt.encode()).hexdigest()
        return self.cache.get(key)

    def cache_response(self, prompt: str, response: str):
        key = hashlib.md5(prompt.encode()).hexdigest()
        self.cache[key] = response

cache = ClientSideCache()

# Utilisation
cached = cache.get_cached_response(user_prompt)
if cached:
    return cached
else:
    response = client.messages.create(...)
    cache.cache_response(user_prompt, response.content[0].text)
    return response
```

**Avantages** :
- Fonctionne toujours
- Contr√¥le total TTL
- Pas de co√ªt additionnel

**Inconv√©nients** :
- Latence non r√©duite (toujours requ√™te API)
- Pas d'√©conomie tokens r√©els

---

## üìä Performance Estim√©e

### Sans Cache

```
Latence : 3000ms
Tokens  : 10,050 input (10K system + 50 user)
```

### Avec Cache (1√®re requ√™te)

```
Latence : 3200ms (+7% write cache)
Tokens  : 10,050 input (cache write)
```

### Avec Cache (requ√™tes suivantes)

```
Latence : 500ms (-85% !)
Tokens  : 50 input (90% reduction)
```

---

## üéì Key Takeaways

1. **Support OAuth tr√®s incertain** (beta feature)
2. **90% r√©duction co√ªt** input tokens (si support√©)
3. **85% r√©duction latence** (cache hit)
4. **TTL 5 minutes** (revalidation apr√®s)
5. **Minimum 1024 tokens** pour cache
6. **System prompt** = use case principal
7. **Test recommand√©** avant d√©ploiement production
8. **Alternative** : Cache c√¥t√© client (toujours fonctionnel)

---

## ‚úÖ Checklist (Si Support√©)

- [ ] Tester support OAuth avec header beta
- [ ] Valider portion cach√©e > 1024 tokens
- [ ] Utiliser `cache_control: {type: ephemeral}`
- [ ] Parser `cache_creation_input_tokens` dans usage
- [ ] Monitorer cache hit rate
- [ ] Documenter TTL (5 min)
- [ ] Fallback si cache non disponible
- [ ] Logger √©conomies tokens

---

## üìö Ressources

### Documentation Officielle
- Prompt Caching : https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
- Beta Features : https://docs.anthropic.com/en/api/versioning

### Comparaison
- **OpenAI** : Pas de prompt caching natif
- **Google** : Context caching (similaire)
- **Anthropic** : Prompt caching (beta API Key)

---

**Derni√®re mise √† jour** : 2025-11-05 17:05
**Confiance** : 35% (extrapol√© API Key, support OAuth tr√®s incertain)
**Action critique** : **TESTER SUPPORT OAUTH** (probablement NON disponible)
**Prochaine √©tape** : Synth√®se finale Session 5
