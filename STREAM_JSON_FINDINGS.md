# stream-json Findings - Test Final

**Date**: 2025-01-07
**Test**: Comportement rÃ©el de `--input-format stream-json` + `--output-format stream-json`

---

## ğŸ¯ DÃ©couverte

`--input-format stream-json` permet effectivement de traiter **plusieurs messages dans le mÃªme processus Claude CLI**, MAIS pas de maniÃ¨re interactive.

---

## ğŸ§ª Test RÃ©alisÃ©

**Command**:
```bash
{
  echo '{"type":"user","message":{"role":"user","content":"Message 1: RÃ©ponds juste OK1"}}'
  echo '{"type":"user","message":{"role":"user","content":"Message 2: RÃ©ponds juste OK2"}}'
  echo '{"type":"user","message":{"role":"user","content":"Message 3: RÃ©ponds juste OK3"}}'
} | claude --print --model haiku --input-format stream-json --output-format stream-json --verbose
```

**RÃ©sultat**:
- âœ… **3 rÃ©ponses reÃ§ues**: `"result":"OK1"`, `"result":"OK2"`, `"result":"OK3"`
- âœ… **MÃªme session_id** pour les 3: `42c2894b-3185-4835-96b2-4960595d6058`
- âœ… **MÃªme processus** Claude CLI
- âœ… **3 requÃªtes API sÃ©parÃ©es** (3 coÃ»ts distincts)
- âœ… **Cache prompt augmente**: 15231 â†’ 32086 â†’ 32105 tokens (efficacitÃ©++)

---

## ğŸ’¡ Comment Ã§a marche

### Mode de fonctionnement

1. **Processus dÃ©marre** : `claude --print --input-format stream-json ...`
2. **Lit stdin complet** : Attend EOF (fermeture stdin)
3. **Parse tous les messages** : Chaque ligne = une conversation indÃ©pendante
4. **Traite sÃ©quentiellement** :
   - Message 1 â†’ RequÃªte API 1 â†’ RÃ©ponse 1
   - Message 2 â†’ RequÃªte API 2 â†’ RÃ©ponse 2
   - Message 3 â†’ RequÃªte API 3 â†’ RÃ©ponse 3
5. **Exit**

### Format stdin correct

```json
{"type":"user","message":{"role":"user","content":"Texte message 1"}}
{"type":"user","message":{"role":"user","content":"Texte message 2"}}
{"type":"user","message":{"role":"user","content":"Texte message 3"}}
```

Chaque ligne = un objet JSON complet.

---

## âš ï¸ Limitation CRITIQUE pour le wrapper

### Ce qu'on voudrait (interactif)

```python
process = start_claude_cli()

# Envoyer message 1
process.stdin.write(msg1)
process.stdin.flush()

# ATTENDRE rÃ©ponse 1
response1 = read_until_complete(process.stdout)

# Envoyer message 2 (basÃ© sur response1)
process.stdin.write(msg2)
process.stdin.flush()

# ATTENDRE rÃ©ponse 2
response2 = read_until_complete(process.stdout)
```

### Ce que stream-json fait rÃ©ellement

```python
process = start_claude_cli()

# Envoyer TOUS les messages d'un coup
process.stdin.write(msg1 + "\n")
process.stdin.write(msg2 + "\n")
process.stdin.write(msg3 + "\n")
process.stdin.close()  # EOF obligatoire

# Processus traite TOUT puis exit
all_responses = process.communicate()
```

**ProblÃ¨me**: On doit connaÃ®tre TOUS les messages **Ã  l'avance** avant de fermer stdin.

---

## ğŸš« Pourquoi Ã§a ne marche pas pour notre use case

Notre wrapper reÃ§oit les requÃªtes HTTP **une par une**, avec des dÃ©lais imprÃ©visibles entre elles:

```
HTTP Request 1 â†’ [Processus Claude CLI] â†’ Response 1
  â±ï¸ 30 secondes d'attente
HTTP Request 2 â†’ [Processus Claude CLI] â†’ Response 2
  â±ï¸ 2 minutes d'attente
HTTP Request 3 â†’ [Processus Claude CLI] â†’ Response 3
```

Avec `stream-json`, il faudrait:
1. Recevoir Request 1
2. **ATTENDRE indÃ©finiment** Request 2 et 3 sans fermer stdin
3. Envoyer tout d'un coup

**Impossible** car:
- On ne sait pas combien de requÃªtes vont arriver
- On ne peut pas laisser stdin ouvert indÃ©finiment
- Le processus ne commence PAS Ã  traiter tant que stdin n'est pas fermÃ©

---

## âœ… Cas d'usage valide

`stream-json` est utile pour **batch processing** oÃ¹ on connaÃ®t toutes les requÃªtes Ã  l'avance:

```bash
# Exemple: Traduire 100 phrases d'un fichier
cat phrases.jsonl | claude --print --input-format stream-json --output-format stream-json

# Contenu phrases.jsonl:
{"type":"user","message":{"role":"user","content":"Traduis: Hello"}}
{"type":"user","message":{"role":"user","content":"Traduis: Goodbye"}}
{"type":"user","message":{"role":"user","content":"Traduis: Thank you"}}
...
```

**Avantage**:
- âœ… **Un seul cold start** pour 100 requÃªtes
- âœ… **Cache prompt** rÃ©utilisÃ© entre messages (~50% Ã©conomie aprÃ¨s message 15)
- âœ… **Streaming** de toutes les rÃ©ponses

---

## ğŸ“Š Comparaison

### Architecture actuelle (subprocess per request)

```
Request 1 â†’ spawn Claude CLI â†’ API call â†’ Response 1 â†’ kill process
  â±ï¸ 5-15s (cold start + API)

Request 2 â†’ spawn Claude CLI â†’ API call â†’ Response 2 â†’ kill process
  â±ï¸ 5-15s (cold start + API)
```

**CoÃ»t**: 2 cold starts, pas de cache

### Avec stream-json (batch)

```
[Envoyer Request 1 + 2 ensemble]
  â†“
spawn Claude CLI
  â†“
API call 1 â†’ Response 1
  â†“
API call 2 â†’ Response 2 (avec cache!)
  â†“
kill process

â±ï¸ 5s cold start + 3.5s API 1 + 1.5s API 2 (cache) = 10s total
```

**CoÃ»t**: 1 cold start, cache rÃ©utilisÃ©

### Avec keep-alive (idÃ©al mais impossible)

```
Request 1 â†’ [Processus existant] â†’ API call â†’ Response 1
  â±ï¸ 0s cold start + 3.5s API = 3.5s

Request 2 â†’ [MÃªme processus] â†’ API call â†’ Response 2
  â±ï¸ 0s cold start + 1.5s API (cache) = 1.5s
```

**CoÃ»t**: 0 cold start, cache rÃ©utilisÃ©

---

## ğŸ¯ Conclusion

### Pour notre wrapper HTTP

`stream-json` **ne rÃ©sout PAS** le problÃ¨me de keep-alive car:

1. âŒ Requiert EOF sur stdin pour dÃ©marrer le traitement
2. âŒ Pas d'interaction en temps rÃ©el (ping-pong)
3. âŒ Impossible de gÃ©rer des requÃªtes HTTP espacÃ©es dans le temps

### Architecture correcte

L'approche actuelle (subprocess.run per request) reste **optimale** pour:
- âœ… RequÃªtes HTTP individuelles
- âœ… DÃ©lais imprÃ©visibles entre requÃªtes
- âœ… Isolation complÃ¨te par utilisateur
- âœ… SimplicitÃ© et robustesse

### Cas d'usage stream-json

Utile uniquement pour:
- âœ… Batch processing de fichiers
- âœ… Pipelines Unix (cat file | claude)
- âœ… Scripts oÃ¹ toutes les requÃªtes sont connues Ã  l'avance

---

## ğŸ“ Format JSON correct

Pour rÃ©fÃ©rence, le format `stream-json` attendu:

```json
{
  "type": "user",
  "message": {
    "role": "user",
    "content": "Texte du message"
  }
}
```

Erreurs courantes:
```json
// âŒ INCORRECT
{"type": "user", "role": "user", "content": "..."}
{"type": "user", "content": "..."}
{"message": {"role": "user", "content": "..."}}

// âœ… CORRECT
{"type": "user", "message": {"role": "user", "content": "..."}}
```

---

**Conclusion finale**: L'architecture actuelle v21 (subprocess per request) est **correcte et optimale** pour notre use case (wrapper HTTP multi-tenant).
